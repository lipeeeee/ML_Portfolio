<div align="center">

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)
![pytorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
<!-- [![Windows](https://img.shields.io/badge/Platform-Windows-0078d7.svg?style=for-the-badge)](https://en.wikipedia.org/wiki/Microsoft_Windows) -->
<!-- [![License](https://img.shields.io/github/license/R3nzTheCodeGOD/R3nzSkin.svg?style=for-the-badge)](LICENSE) -->

# **PPO** Agent playing **LunarLander-v2**
</div>

This is a trained model of a **PPO** agent playing **LunarLander-v2**
using the [stable-baselines3 library](https://github.com/DLR-RM/stable-baselines3).


https://user-images.githubusercontent.com/62669782/225029816-ea96b0ea-5d85-47ad-b2ef-3872b4c93c9f.mp4


## Model
In this repository you will find the PPO model/agent(not the notebook) that I made specifically to play **LunarLander-v2**, even though I trained it on **2 million** timesteps, It still had room for improvement with more timesteps.

## Metrics
My model was in the top 268/3031 in the
[Hugging Face Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard), with a:
- Mean Reward: <b>283.98</b>
- Std Reward: <b>15.75</b>
- Result: <b>268.23</b>

(check metrics on *results.json*)
## PPO
[Proximal Policy Optimization (PPO)](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b) is a SOTA Reinforcement Learning algorithm developed by OpenAI.

<br>

*a model by lipeeeee*
